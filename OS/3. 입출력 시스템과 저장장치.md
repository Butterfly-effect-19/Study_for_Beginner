# 입출력 시스템과 저장장치

작성일시: 2021년 6월 11일 오전 10:02
최종 편집일시: 2021년 6월 11일 오전 11:16

# 01. 입출력 시스템

입출력 장치 = 주변장치

저속 주변 장치 : 메모리와 주변장치 사이에 오고 가는 데이터의 양이 적어 데이터 전송률이 낮은 장치

고속 주변 장치 : 메모리와 주변장치 사이에 대용량의 데이터가 오고 가 데이터 전송률이 높은 장치


## 입출력 버스의 구조

---

### 초기

![Untitled 1](https://user-images.githubusercontent.com/52151533/121624809-2e4ec800-caad-11eb-908a-28fa2c2fe9c7.png)



모든 장치가 하나의 버스로 연결. 

CPU가 작업을 진행하다 입출력 명령을 만나면 직접 입출력장치에서 데이터를 가져오는 "폴링"방식 이용

### 입출력 제어기 사용
![Untitled 2](https://user-images.githubusercontent.com/52151533/121624901-5fc79380-caad-11eb-933d-66f8b91a4293.png)

입출력 제어기 : 2개의 채널(메인 버스와 입출력 버스)로 나뉨. 

- 메인버스 : 고속으로 작동하는 CPU와 메모리가 사용
- 입출력 버스 : 주변 장치가 사용

입출력 제어기를 사용하면 느린 입출력 장치로 인해 CPU의 메모리 작업이 느려지는 것을 막을 수 있어 전체 작업 효율 향상

### 입출력 버스의 분리
![Untitled 3](https://user-images.githubusercontent.com/52151533/121624912-66560b00-caad-11eb-81b5-bcba3d7fd33d.png)



입출력 제어기를 사용하면 작업 효율을 높일 수 있지만, 저속 주변장치 때문에 고속 주변장치의 데이터 전송 느려짐 

→ 고속 입출력 버스와 저속 입출력 버스 분리 

→ 두 버스 사이의 데이터 전송은 채널 선택기가 관리 

(결론) 현대의 컴퓨터는 메인 버스, 그래픽 버스, 고속 입출력 버스, 저속 입출력 버스 사용.

## 직접 메모리 접근 DMA

---

- CPU의 도움 없이 메모리에 접근할 수 있도록 입출력 제어기에 부여된 권한.
- 입출력 제어기에는 직접 메모리에 접근하기 위한 DMA 제어기가 마련됨.

### 메모리 공간 분할

CPU의 작업 공간과 DMA의 작업 공간이 겹치는 것을 방지하기 위해 공간을 분리하여 메인 메모리 운영 ⇒ 메모리 맵 입출력 memory mapped I/O

## 인터럽트

---

: 주변장치의 입출력 요구나 하드웨어의 이상 현상을 CPU에 알려주는 역할을 하는 신호

, 각 장치에 IRQ라는 고유의 인터럽트 번호 부여


|종류|특징|비고|
|------|--------------|----------|
|외부 인터럽트|입출력 및 하드웨어 관련 인터럽트|주변장치의 변화, 하드웨어 이상|
|내부 인터럽트|프로세스의 오류로 발생하는 인터럽트|예외 상황 인터럽트|
|시그널|사용자의 요청으로 발생하는 인터럽트|자발적 인터럽트|

![Untitled 4](https://user-images.githubusercontent.com/52151533/121624927-6d7d1900-caad-11eb-893f-658fe3192507.png)


인터럽트 벡터 : 여러 인터럽트 중 어떤 인터럽트가 발생했는지 파악하기 위해 사용하는 자료구조

(값이 1 : 인터럽트 발생)

인터럽트 핸들러 : 인터럽트의 처리 방법을 함수 형태로 만들어 놓은 것. 

# 02. 디스크 장치

## 디스크 장치의 종류

### 하드디스크

---

![Untitled 5](https://user-images.githubusercontent.com/52151533/121624947-7837ae00-caad-11eb-9491-66fe4991a78e.png)
- 플래터
    - 표면에 자성체가 발려 있어 자기를 이용하여 0과 1의 데이터 저장
    - N극 0, S극  1
- 섹터와 블록
    - 섹터
        - 하드디스크의 가장 작은 저장 단위
        - 하나의 섹터에는 한 덩어리의 데이터가 저장
    - 블록
        - 하드디스크와 컴퓨터 사이에 데이터를 전송하는 논리적 저장 단위 중 가장 작은 단위
        - 블록은 여러 개의 섹터로 구성되며, 윈도우 O.S에서는 블록 대신 클러스터라고 표현

    ⇒ 하드디스크 입장 : 섹터가 가장 작은 저장 단위

    ⇒ 운영체제 입장 : 블록이 가장 작은 저장 단위

- 트랙과 실린더
    - 트랙: 플래터에서 회전 축을 중심으로 데이터가 기록되는 동심원, 즉 동일한 동심원 상에 있는 섹터의 집합
    - 실린더 : 여러개의 플래터에 있는 같은 트랙의 집합
- 헤드와 플래터
    - 하드디스크에서 데이터를 읽거나 쓸 때는 읽기/ 쓰기 헤드 사용

### CD

---

- 휴대할 수 있는 소형 원반에 데이터 저장
- 하드디스크와 마찬가지로 트랙과 섹터로 구성
- 표면에 미세한 홈이 파여있어 헤드에서 발사된 레이저가 홈에 들어가 반사가 되지 않으면 0으로, 반사되어 돌아오면 1로 인식.

### 하드디스크 VS CD

---

![Untitled 6](https://user-images.githubusercontent.com/52151533/121625019-9bfaf400-caad-11eb-8585-786080dce0a0.png)
- 각속도 일정 방식
    - 하드디스크의 플래터는 항상 일정한 속도로 회전하여 바깥쪽 트랙의 속도가 안쪽 트랙의 속도보다 훨씬 빠르므로 가장 바깥쪽에 있는 섹터가 가장 안쪽에 있는 섹터보다 더 큰데, 일정한 시간 동안 이동한 각도가 같다는 의미에서 이러한 방식을 각속도 일정이라고 함
    - 트랙마다 속도가 다름 → 섹터의 크기도 다름
- 선속도 일정 방식
    - CD에서 이용
    - 어느 트랙에서나 단위시간당 디스크의 이동 거리가 같은데 이를 구현하려면 헤드가 안쪽 트랙에 있을 때는 디스크의 회전 속도를 빠르게 하고, 헤드가 바깥쪽 트랙으로 이동했을 때는 디스크의 회전 속도를 느리게 해야  함.
    - 모든 트랙의 움직이는 속도도 같고 섹터의 크기도 같아서 안쪽 트랙보다 바깥쪽 트랙에 더 많은 섹터가 존재

## 디스크 장치의 데이터 전송시간

---

- 탐색시간 : 헤드가 현재 위치에서 그 트랙까지 이동하는 데 걸리는 시간
- 회전지연시간 : 원하는 섹터를 만날 때까지 회전하는 데 걸리는 시간
- 전송시간 : 데이터 전송에 걸리는 시간

$$데이터 전송 시간 = 탐색시간 + 회전 지연 시간 + 전송시간$$

**하드디스크의 성능을 높이려면 탐색 시간 최소화해야 함!! → 디스크 스케줄링 기법 사용**

## 디스크장치관리

---

- 파티션
    - 디스크를 논리적으로 분할하는 작업
    - 대용량 하드스크의 경우 하나로 사용하기보다 여러 개로 나누어 사용하면 관리하기 편함
    - 마운트
        - 여러개의 하드디스크를 하나의 파티션으로 통합하는 기능 in 유닉스 운영체제
- 포매팅
    - 디스크에 파일 시스템을 탑재하고 디스크 표면을 초기화하여 사용할 수 있는 형태
    - 빠른 포매팅 vs 느린 포매팅

        빠른 포매팅 : 데이터는 그대로 둔 채 파일 테이블을 초기화하는 방식

        느린 포매팅 : 디스크에 파일 시스템을 탑재하고 디스크 표면을 초기화하는 방식

- 조각모음
    - 하드디스크에 조각이 많이 생기면 큰 파일이 여러 조각으로 나뉘어 저장되고 이를 읽기 위해 하드디스크의 여러 곳을 돌아다녀야 하기 때문에 성능 저하로 이어짐

## 네트워크 저장장치

---

- DAS
    - **서버**와 같은 컴퓨터에 **직접** **연결**된 저장장치를 사용하는 방식
    - = HAS
    - 단점 : 컴퓨터에 직접 연결된 저장장치를 사용하기 때문에 다른 운영체제가 쓰는 파일  시스템 사용할 수 없음.
- NAS
    - 기존의 저장장치를 **LAN이나 WAN**에 붙여서 사용하는 방식
    - 저장장치를 네트워크상에 두고 여러 클라이언트가 네트워크를 통해 접근하게 함으로써 공유 데이터 관리 및 데이터의 중복 회피 가능
- SAN
    - 데이터 서버, 백업 서버, RAID 등의 장치를 **네트워크**로 묶고 **데이터 접근을 위한 서버**를 두는 형태
    - 시스템이 제공하는 인터페이스를 통해 데이터에 접근
    - 저장장치에 필요한 장치를 네트워크로 묶어 하나의 시스템을 구성 → 다양한 서비스 제공
    - 데이터의 공유, 백업, 보안 등이 서버를 통해 자동으로 이루어짐

   
## 3. 디스크 스케줄링

디스크 스케줄링 : 트랙의 이동을 최소화하여 **탐색 시간**을 줄이는 것이 목적

트랙은 0에서 24까지 총 25개로 구성

디스크 스케줄링 기법의 성능을 비교하는 지표는 **트랙의 총 이동거리**

가장 단순한 방식에서 시작하여 그것의 단점을 개선한 방식순

- 디스크 스케줄링 기법
- FCFS

    가장 단순한 디스크 스케줄링 방식으로, 트랙 요청이 들어온 순서대로 서비스한다.

    `헤드가 이동한 총거리` : 7+9+6+8+20++5+6=65

    ![fcfs](https://github.com/Butterfly-effect-19/Study_for_Beginner/blob/main/image/fcfs.png)

- SSTF

    현재 헤드가 있는 위치에서 가장 가까운 트랙부터 서비스한다. 만약 다음에 서비스할 두 트랙의 거리가 같다면 먼저 요청 받은 트랙을 서비스한다.

    15번 트랙을 서비스한 후 디스크 서비스 큐를 검색하여 15번 트랙과 가장 가까이에 있는 14번 트랙으로 이동

    `헤드가 이동한 총거리` : 31

    효율성은 좋지만 아사 형상을 일으킬 수 있어 사용하지 않음

    ![sstf](https://github.com/Butterfly-effect-19/Study_for_Beginner/blob/main/image/sstf.png)

- 블록 SSTF

    SSTF 디스크 스케줄링의 공평성 위배를 어느 정도 해결한 방법으로, SSTF 디스크 스케줄링에 에이징을 적용한 것이다.

    큐에 있는 트랙 요청을 일정한 블록 형태로 묶는다.

    ![block_sstf](https://github.com/Butterfly-effect-19/Study_for_Beginner/blob/main/image/blocksstf.png)

    블록의 크기를 3으로 설정해 현재 트랙에서 가장 먼 트랙을 블록의 끝에 위치

    따라서 멀리 있는 트랙도 몇 번만 양보하면 서비스 받을 수 있음

    `헤드가 이동한 총거리` : 51

    ![block_sstf2](https://github.com/Butterfly-effect-19/Study_for_Beginner/blob/main/image/blocksstf2.png)

    블록 SSTF 디스크 스케줄링은 에이징을 사용하여 공평성을 보장하지만 성능은 FCFS만큼 좋지 않음

- SCAN (=엘리베이터 기법)

    SSTF 디스크 스케줄링의 공평성 위배 문제를 완화하기 위해 만들어진 기법으로, 헤드가 움직이기 시작하면 맨 마지막 트랙에 도착할 때까지 뒤돌아가지 않고 계속 앞으로만 전진하면서 요청 받은 트랙을 서비스한다.

    `헤드가 이동한 총거리` : 38

    ![scan](https://github.com/Butterfly-effect-19/Study_for_Beginner/blob/main/image/scan.png)

    동일한 트랙이나 실린더 요청이 연속적으로 발생하면 헤드가 더 이상 나아가지 못하고 제자리에 머물게 되어 바깥쪽 트랙이 아사 현상을 겪는 문제가 발생

- C-SCAN

    SCAN 디스크 스케줄링을 변형한 것 이다.

    헤드가 한쪽 방향으로 움직일 때는 요청 받은 트랙을 서비스하지만 반대 방향으로 돌아올 때는 서비스하지 않고 헤드만 이동한다.

    `헤드가 이동한 총거리` : 46

    ![c-scan](https://github.com/Butterfly-effect-19/Study_for_Beginner/blob/main/image/c-scan.png)

    모든 트랙의 방문 횟수가 동일하여 공평하게 서비스 받는다. 그러나 작업 없이 헤드를 이동하는 것은 매우 비효율적이다.

    동일한 트랙 요청이 연속적으로 발생하면 바깥쪽 트랙이 아사 현상을 겪는다.

    따라서 C-SCAN 디스크 스케줄링은 잘 사용되지 않는다.

- LOOK

    SCAN 디스크 스케줄링의 불필요한 부분을 제거하여 효율을 높였다.

    더 이상 서비스할 트랙이 없으면 헤드가 끝까지 가지 않고 중간에서 방향을 바꾼다.

    `헤드가 이동한 총거리` : 35

    ![look](https://github.com/Butterfly-effect-19/Study_for_Beginner/blob/main/image/look.png)

    LOOK 디스크 스케줄링은 많이 사용된다.

- C-LOOK

    C-SCAN 디스크 스케줄링의 LOOK 버전이다. 더 이상 서비스할 트랙이 없으면 헤드가 중간에서 방향을 바꾼다.

    `헤드가 이동한 총거리` : 38

    ![c-look](https://github.com/Butterfly-effect-19/Study_for_Beginner/blob/main/image/c-look.png)

- SLTF (=최소 지연 우선 기법)

    헤드가 고정된 저장장치에서 사용하는 스케줄링 기법으로, 작업 요청이 들어온 섹터의 순서를 디스크가 회전하는 방향에 맞추어 다시 정렬한 후 서비스한다.

    ![sltf](https://github.com/Butterfly-effect-19/Study_for_Beginner/blob/main/image/sltf.png)

    헤드가 고정된 저장장치에 적용되는 SLTF는 매우 고가라 많이 사용되지 않는다.

---

## 4. RAID

여러개의 디스크를 배열하여 **속도의 증대, 안정성의 증대, 효율성, 가용성의 증대**를 이루는 기술

자동으로 백업을 하고 장애가 발생하면 이를 복구하는 시스템으로, 동일한 규격의 디스크를 여러 개 모아 구성하며 장애가 발생했을 때 데이터를 복구하는 데 사용된다. 디스크를 구성하는 방식에 따라 RAID 0, 1, 2, 3, 4, 5, 6, 10, 50, 60 등이 있다.

- 신뢰도를 높이는 방법
    - `미러링` : 2개의 디스크에 거울처럼 똑같은 내용을 저장함
    - `패리티 디스크` : 디스크 레이드 4개 중 하나를 백업 디스크로 두는 것. 1개의 디스크 고장까지는 대처가 가능하다.
    - `오류 교정 코드 ECC` : 미러링과 패러티 디스크 사이의 절충안, 패러티디스크 보다 백업디스크를 더 두고, 이를 해밍코드 방식으로 동작하게 함으로써 커버한다. <- 이제는 잘 사용되지 않음.

1. **RAID 0 (순수한 스트라이핑)**
    - 병렬로 연결된 여러 개의 디스크에 데이터를 동시에 입출력 할 수 있도록 구성
    - 데이터를 여러 조각으로 나누어 보냄
    - 4개의 디스크로 구성된 RAID 0은 1개의 디스크로 구성된 일반 시스템보다 이론적으로는 입출력 속도가 4배 빠름
    - 장애 발생 시 복구하는 기능이 없기 때문에 데이터를 잃는다.
    - 입출력이 빨라 기업용 제품, 개인용 컴퓨터와 노트북의 고급 기종에 많이 사용하기도 함

2. **RAID 1 (순수한 미러링)**
    - 하나의 데이터를 2개의 디스크에 나누어 저장하여 장애 시 백업 디스크로 활용
    - 데이터가 똑같이 여러 디스크에 복사 됨
    - 같은 크기의 디스크를 최소 2개 이상 필요로 하며 짝수 개의 디스크로 구성
    - 저장하는 데이터와 같은 디스크가 하나 더 필요하기 때문에 비용 증가
    - 같은 내용을 두 번 저장하기 때문에 속도가 느려질 수 있음

3. **RAID 10 (RAID 1+0)**
    - RAID 0과 RAID 1을 결합한 형태
    - RAID 1로 먼저 묶음
    - 장애가 발생했을 경우 일부 디스크만 중단하여 복구할 수 있음

    `패리티 비트` : 오류 검출 코드, 오류가 발생했는지 확인할 수 있는 코드
    `허밍 코드` : 오류 교정 코드, 오류가 발생했는지 확인하고 오류를 교정할 수 있는 코드

    RAID 2, 3, 4, 5, 6, 50, 60 등은 오류 교정 코드(허밍 코드), 오류 검출 코드(패리티 코드)를 이용하여 추가되는 디스크의 양을 줄이는 방식

    하드디스크의 가격이 저렴해지면서 복잡한 연산을 필요로 하는 RAID 2, 3, 4, 5를 잘 사용하지 않게 됨.

4. **RAID 2**
    - 오류를 검출하는 기능이 없는 디스크에 대해 오류 교정 코드를 따로 관리
    - 오류 교정 코드(허밍 코드) 를 별도의 디스크에 따로 보관하고 있다가 장애가 발생하면 이를 이용하여 데이터를 복구
    - n개의 디스크에 대해 오류 교정 코드를 저장하기 위한 n-1개의 추가 디스크를 필요로 하므로 RAID 1 보다는 작은 저장 공간을 요구하지만 오류 교정 코드를 계산하는 데 많은 시간을 소비하여 잘 사용되지 않음

5. **RAID 3**
    - 패리티 비트를 사용하여 데이터를 복구
    - 섹터 단위로 데이터를 나누어 저장
    - N-way 패리티 방식 : 패리티 비트를 여러 섹터끼리 묶어서 구성하면 오류가 없는 섹터를 이용해 오류가 있는 섹터의 데이터를 복원 가능
    - 추가되는 디스크의 양은 4개의 디스크당 1개 정도
    - N-way 패리티 비트를 구성하는 데 필요한 계산량이 많다는 것이 단점

6. **RAID 4**
    - 데이터를 하나의 디스크에 블록 단위로 저장하고 패리티 비트를 블록과 연결하여 구성
    - 데이터가 저장되는 디스크와 패리티 비트가 저장되는 디스크만 동작한다는 것이 장점
    - RAID 3과 마찬가지로 패리티 비트를 추가하기 위한 계산량이 많지만 추가되는 디스크의 양은 적음
    - RAID 4를 사용하지 않는 이유는 데이터의 병목 현상 때문

7. **RAID 5**
    - 패리티 비트를 여러 디스크에 분산하여 보관함으로써 패리티 비트 디스크의 병목 현상을 완화
    - 패리티 비트를 해당 데이터가 없는 디스크에 보관
    - 한 디스크에 장애가 발생하면 다른 디스크에 있는 패리티 비트를 이용하여 데이터를 복구할 수 있음
    - 디스크 2개에 동시에 장애가 발생했을 때는 복구가 불가능

8. **RAID 6**
    - RAID 5와 같은 방식이지만 패리티 비트가 2개여서 디스크 2개의 장애를 복구할 수 있음
    - 패리티 비트를 2개씩 운영하기 때문에 RAID 5보다 계산량이 많고 4개의 디스크당 2개의 추가 디스크가 필요하다는 단점이 있음

9. **RAID 50**
    - RAID 5로 묶은 두 쌍을 다시 RAID 0으로 묶어 사용

10. **RAID 60**
    - RAID 6로 묶은 두 쌍을 다시 RAID 0으로 묶어 사용

RAID 50, 60의 장단점은 RAID 5, 6의 장단점과 같음
RAID 50과 RAID 60은 RAID 10에 비해 추가되는 디스크의 수가 적지만 입출력 시 계산량이 증가

---

## 5. [심화학습] 하드웨어의 규격과 발전

### 1. 포트의 규격

- CPU 포트 : CPU를 꽂는 곳. CPU 포트 주위에는 CPU를 냉각하는 팬을 설치하기 위한 공간이 따로 마련되어 있다.
- 램 포트 : 램을 수직으로 꽂는 곳. 용량이 같더라도 다른 규격의 램을 꽂으면 낮은 속도의 램을 기준으로 성능이 떨어짐
- 그래픽 포트 : 외부 그래픽 카드를 연결하는 포트.
- SATA : 하드디스크 같은 저장장치를 연결하는 직렬 ATA 포트. SSD, CD-ROM 등의 저장장치를 연결하기 위해 사용. 여러 대의 하드디스크, SSD, CD-ROM이 하나의 SATA 버스를 공유하는 구조. 높은 데이터 전송률과 빠른 응답성을 필요로 하는 그래픽카드는 SATA에 묶어 사용하지 않고 독자적인 버스를 사용하여 CPU에 직접 연결함.
- PCI : 그 외의 주변장치는 메인보드에 주변장치를 연결하는 포트. 최신 메인보드에는 그래픽카드, 사운드카드, 랜카드가 내장되어 있어 PCI 버스에 연결할 것이 많지 않음.

## 🔗출처

---

- 쉽게 배우는 운영체제(한빛아카데미) - 10장